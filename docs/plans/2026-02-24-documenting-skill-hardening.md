# Documenting Skill Hardening — Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Harden the `/documenting` skill with drift detection, a Stop hook for auto-triggering, intra-memory auto-index for efficient navigation, and a clear SKILL.md workflow.

**Architecture:** Add `tracks:` frontmatter to memories → `drift_detector.py` cross-checks git-modified files against those globs → lightweight Stop hook fires the detector at session end → `validate_metadata.py` generates a `<!-- INDEX -->` TOC inside each memory on every validation run.

**Tech Stack:** Python 3 stdlib only (`pathlib`, `re`, `yaml`, `subprocess`, `json`, `fnmatch`), pytest for tests.

**Design doc:** `docs/plans/2026-02-24-documenting-skill-hardening-design.md`

---

## Task 1: Add `tracks:` field to schema and templates

**Files:**
- Modify: `skills/documenting/references/metadata-schema.md`
- Modify: `skills/documenting/scripts/generate_template.py`

**Step 1: Add `tracks:` to the metadata schema**

In `metadata-schema.md`, add a new optional field to the SSOT section table after `changelog`:

```markdown
| `tracks` | array | Optional | Glob patterns for files this memory documents. Used by drift_detector. |
```

And add to the `## Example Frontmatter` SSOT block:

```yaml
tracks:
  - "src/analytics/**/*.py"
  - "src/analytics/config.yaml"
```

**Step 2: Add `tracks:` and INDEX stub to SSOT template in `generate_template.py`**

In the `TEMPLATES["ssot"]` string, replace the frontmatter block:

```python
    "ssot": """---
title: {title}
version: 0.1.0
updated: {timestamp}
scope: {scope}
category: {category}
subcategory: {subcategory}
domain: [{domain}]
applicability: {applicability}
tracks: []
changelog:
  - 0.1.0 ({date}): Initial creation.
---

<!-- INDEX: auto-generated by validate_metadata.py — do not edit manually -->
| Section | Summary |
|---|---|
| [Purpose](#purpose) | _run validate_metadata.py to generate_ |
<!-- END INDEX -->

## Purpose
{purpose}
...
```

Apply same INDEX stub (after `---`) to `TEMPLATES["pattern"]` and `TEMPLATES["reference"]` and `TEMPLATES["plan"]`.

**Step 3: Commit**

```bash
git add skills/documenting/references/metadata-schema.md \
        skills/documenting/scripts/generate_template.py
git commit -m "feat(documenting): add tracks: field to schema and index stub to templates"
```

---

## Task 2: INDEX generation in `validate_metadata.py`

**Files:**
- Modify: `skills/documenting/scripts/validate_metadata.py`
- Test: `skills/documenting/tests/test_validate_metadata.py` (create)

**Step 1: Write failing tests**

Create `skills/documenting/tests/test_validate_metadata.py`:

```python
import pytest
from pathlib import Path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "scripts"))
from validate_metadata import extract_headings, generate_index_table, inject_index


SAMPLE = """---
title: Test
version: 1.0.0
updated: 2026-02-24
scope: test
category: ssot
subcategory: test
domain: [test]
---

<!-- INDEX: auto-generated by validate_metadata.py — do not edit manually -->
| Section | Summary |
|---|---|
| old | old |
<!-- END INDEX -->

## Architecture
Handles the core routing logic for all requests.

## Hook Wiring
Three hooks fire on session events.

## Installer
Single-purpose script, idempotent.
"""


def test_extract_headings():
    headings = extract_headings(SAMPLE)
    assert headings == [
        ("Architecture", "Handles the core routing logic for all requests."),
        ("Hook Wiring", "Three hooks fire on session events."),
        ("Installer", "Single-purpose script, idempotent."),
    ]


def test_generate_index_table():
    headings = [("Architecture", "Core routing."), ("Installer", "Idempotent.")]
    table = generate_index_table(headings)
    assert "| [Architecture](#architecture) |" in table
    assert "Core routing." in table
    assert "| [Installer](#installer) |" in table


def test_inject_index_replaces_existing():
    new_table = "| [Architecture](#architecture) | Core. |\n"
    result = inject_index(SAMPLE, new_table)
    assert "| old | old |" not in result
    assert "| [Architecture](#architecture) | Core. |" in result


def test_inject_index_adds_when_missing():
    content = "---\ntitle: T\n---\n\n## Foo\nBar baz.\n"
    new_table = "| [Foo](#foo) | Bar baz. |\n"
    result = inject_index(content, new_table)
    assert "<!-- INDEX:" in result
    assert "| [Foo](#foo) | Bar baz. |" in result
```

**Step 2: Run tests to verify they fail**

```bash
cd skills/documenting && python -m pytest tests/test_validate_metadata.py -v
```

Expected: `ImportError` — functions don't exist yet.

**Step 3: Implement the three functions in `validate_metadata.py`**

Add after the existing imports and before `REQUIRED_FIELDS`:

```python
import fnmatch


def extract_headings(content: str) -> list[tuple[str, str]]:
    """Extract (heading, first_sentence) for every ## section."""
    results = []
    lines = content.splitlines()
    i = 0
    while i < len(lines):
        line = lines[i]
        if line.startswith("## ") and not line.startswith("### "):
            heading = line[3:].strip()
            # Find first non-empty, non-heading, non-code-fence line
            summary = ""
            j = i + 1
            in_code = False
            while j < len(lines):
                l = lines[j].strip()
                if l.startswith("```"):
                    in_code = not in_code
                if not in_code and l and not l.startswith("#") and not l.startswith("|") and not l.startswith("-"):
                    # Take up to first sentence (period, or whole line if none)
                    sentence = l.split(".")[0].strip()
                    summary = sentence[:120]  # cap length
                    break
                j += 1
            results.append((heading, summary))
        i += 1
    return results


def generate_index_table(headings: list[tuple[str, str]]) -> str:
    """Generate markdown table rows from headings list."""
    rows = ["| Section | Summary |", "|---|---|"]
    for heading, summary in headings:
        anchor = heading.lower().replace(" ", "-").replace("/", "").replace("(", "").replace(")", "")
        rows.append(f"| [{heading}](#{anchor}) | {summary or '_no summary_'} |")
    return "\n".join(rows) + "\n"


def inject_index(content: str, table: str) -> str:
    """Replace or insert INDEX block in memory content."""
    import re
    header = "<!-- INDEX: auto-generated by validate_metadata.py — do not edit manually -->\n"
    footer = "<!-- END INDEX -->"
    block = f"{header}{table}{footer}"

    # Replace existing block
    pattern = r"<!-- INDEX:.*?-->.*?<!-- END INDEX -->"
    if re.search(pattern, content, re.DOTALL):
        return re.sub(pattern, block.rstrip(), content, flags=re.DOTALL)

    # Insert after frontmatter closing ---
    fm_end = re.search(r"^---\n", content[3:], re.MULTILINE)
    if fm_end:
        insert_pos = fm_end.end() + 3  # +3 for the initial ---
        return content[:insert_pos] + "\n" + block + "\n" + content[insert_pos:]

    # Fallback: prepend
    return block + "\n" + content
```

**Step 4: Wire into `validate_metadata()` function**

At the end of `validate_metadata()`, just before `return len(errors) == 0`, add:

```python
    # Regenerate INDEX block if validation passed
    if len(errors) == 0:
        headings = extract_headings(content)
        if headings:
            table = generate_index_table(headings)
            new_content = inject_index(content, table)
            if new_content != content:
                path.write_text(new_content, encoding="utf-8")
                print("  ✏️  INDEX regenerated.")
```

**Step 5: Run tests**

```bash
cd skills/documenting && python -m pytest tests/test_validate_metadata.py -v
```

Expected: all 4 pass.

**Step 6: Commit**

```bash
git add skills/documenting/scripts/validate_metadata.py \
        skills/documenting/tests/test_validate_metadata.py
git commit -m "feat(documenting): auto-generate intra-memory INDEX on validate"
```

---

## Task 3: Create `drift_detector.py`

**Files:**
- Create: `skills/documenting/scripts/drift_detector.py`
- Test: `skills/documenting/tests/test_drift_detector.py` (create)

**Step 1: Write failing tests**

Create `skills/documenting/tests/test_drift_detector.py`:

```python
import pytest
import json
from pathlib import Path
from unittest.mock import patch, MagicMock
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "scripts"))
from drift_detector import extract_tracks, match_files_to_tracks, format_scan_report


MEMORY_WITH_TRACKS = """---
title: Test SSOT
version: 1.0.0
updated: 2026-02-01
tracks:
  - "cli/src/**/*.ts"
  - "hooks/**/*.py"
---

## Purpose
Test memory.
"""

MEMORY_NO_TRACKS = """---
title: No Tracks
version: 1.0.0
updated: 2026-02-01
---

## Purpose
No tracking.
"""


def test_extract_tracks_finds_globs():
    tracks = extract_tracks(MEMORY_WITH_TRACKS)
    assert tracks == ["cli/src/**/*.ts", "hooks/**/*.py"]


def test_extract_tracks_empty_when_missing():
    tracks = extract_tracks(MEMORY_NO_TRACKS)
    assert tracks == []


def test_match_files_to_tracks_hit():
    files = ["cli/src/core/diff.ts", "hooks/skill-suggestion.py"]
    tracks = ["cli/src/**/*.ts", "hooks/**/*.py"]
    matched = match_files_to_tracks(files, tracks)
    assert "cli/src/core/diff.ts" in matched
    assert "hooks/skill-suggestion.py" in matched


def test_match_files_to_tracks_miss():
    files = ["docs/README.md"]
    tracks = ["cli/src/**/*.ts"]
    assert match_files_to_tracks(files, tracks) == []


def test_format_scan_report_stale():
    stale = {"my_memory": {"files": ["cli/src/core/diff.ts"], "updated": "2026-02-01"}}
    report = format_scan_report(stale)
    assert "my_memory" in report
    assert "cli/src/core/diff.ts" in report


def test_format_scan_report_clean():
    report = format_scan_report({})
    assert "clean" in report.lower() or "no stale" in report.lower() or "up to date" in report.lower()
```

**Step 2: Run to verify failure**

```bash
cd skills/documenting && python -m pytest tests/test_drift_detector.py -v
```

Expected: `ImportError` — module doesn't exist.

**Step 3: Implement `drift_detector.py`**

Create `skills/documenting/scripts/drift_detector.py`:

```python
#!/usr/bin/env python3
"""
Detect documentation drift between Serena memories and git-modified files.

Subcommands:
  scan [--since N]     — scan all memories, report stale ones (default: last 30 commits)
  check <memory>       — check a single memory by name
  hook                 — Stop hook mode: check session writes, output JSON if stale
"""

import sys
import re
import json
import fnmatch
import subprocess
from pathlib import Path

import yaml


# ── Path resolution ──────────────────────────────────────────────────────────

def find_project_root() -> Path:
    """Walk up from cwd to find .serena/memories/."""
    p = Path.cwd()
    for parent in [p, *p.parents]:
        if (parent / ".serena" / "memories").exists():
            return parent
    return p


def get_memories_dir(project_root: Path) -> Path:
    return project_root / ".serena" / "memories"


# ── Frontmatter parsing ───────────────────────────────────────────────────────

def extract_frontmatter(content: str) -> dict:
    match = re.match(r"^---\n(.*?)\n---\n", content, re.DOTALL)
    if not match:
        return {}
    try:
        return yaml.safe_load(match.group(1)) or {}
    except yaml.YAMLError:
        return {}


def extract_tracks(content: str) -> list[str]:
    """Return tracks: glob list from memory frontmatter."""
    fm = extract_frontmatter(content)
    tracks = fm.get("tracks", [])
    return tracks if isinstance(tracks, list) else []


def extract_updated(content: str) -> str:
    fm = extract_frontmatter(content)
    return str(fm.get("updated", ""))


# ── File matching ─────────────────────────────────────────────────────────────

def match_files_to_tracks(files: list[str], tracks: list[str]) -> list[str]:
    """Return files that match any of the tracks globs."""
    matched = []
    for f in files:
        for pattern in tracks:
            # Normalize ** to work with fnmatch
            if fnmatch.fnmatch(f, pattern) or fnmatch.fnmatch(f, pattern.replace("**", "*")):
                matched.append(f)
                break
    return matched


# ── Git helpers ───────────────────────────────────────────────────────────────

def get_recent_modified_files(project_root: Path, since_n_commits: int = 30) -> list[str]:
    """Get files modified in the last N commits."""
    try:
        result = subprocess.run(
            ["git", "log", f"-{since_n_commits}", "--name-only", "--format="],
            cwd=project_root, capture_output=True, text=True
        )
        return [l.strip() for l in result.stdout.splitlines() if l.strip()]
    except Exception:
        return []


def get_session_written_files(project_root: Path) -> list[str]:
    """Get files modified in the current session (git diff HEAD)."""
    try:
        result = subprocess.run(
            ["git", "diff", "HEAD", "--name-only"],
            cwd=project_root, capture_output=True, text=True
        )
        uncommitted = [l.strip() for l in result.stdout.splitlines() if l.strip()]
        result2 = subprocess.run(
            ["git", "diff", "--cached", "--name-only"],
            cwd=project_root, capture_output=True, text=True
        )
        staged = [l.strip() for l in result2.stdout.splitlines() if l.strip()]
        return list(set(uncommitted + staged))
    except Exception:
        return []


# ── Core logic ────────────────────────────────────────────────────────────────

def scan_memories(project_root: Path, since_n_commits: int = 30) -> dict:
    """Return dict of {memory_name: {files, updated}} for stale memories."""
    memories_dir = get_memories_dir(project_root)
    if not memories_dir.exists():
        return {}

    modified_files = get_recent_modified_files(project_root, since_n_commits)
    stale = {}

    for md_file in sorted(memories_dir.glob("*.md")):
        content = md_file.read_text(encoding="utf-8")
        tracks = extract_tracks(content)
        if not tracks:
            continue
        matched = match_files_to_tracks(modified_files, tracks)
        if matched:
            updated = extract_updated(content)
            stale[md_file.stem] = {"files": matched[:5], "updated": updated}

    return stale


def format_scan_report(stale: dict) -> str:
    if not stale:
        return "[Docs Drift] All memories up to date. No action needed."

    lines = [f"[Drift Report] {len(stale)} memor{'y' if len(stale)==1 else 'ies'} stale:\n"]
    for name, info in stale.items():
        lines.append(f"  {name}")
        lines.append(f"    Last updated: {info['updated']}")
        for f in info["files"][:3]:
            lines.append(f"    Modified: {f}")
        lines.append("")
    lines.append("Run /documenting to update.")
    return "\n".join(lines)


# ── Subcommands ───────────────────────────────────────────────────────────────

def cmd_scan(args: list[str]):
    since = 30
    if "--since" in args:
        idx = args.index("--since")
        since = int(args[idx + 1]) if idx + 1 < len(args) else 30

    project_root = find_project_root()
    stale = scan_memories(project_root, since)
    print(format_scan_report(stale))
    sys.exit(1 if stale else 0)


def cmd_check(args: list[str]):
    if not args:
        print("Usage: drift_detector.py check <memory-name>")
        sys.exit(1)

    memory_name = args[0]
    project_root = find_project_root()
    memories_dir = get_memories_dir(project_root)
    md_file = memories_dir / f"{memory_name}.md"

    if not md_file.exists():
        print(f"Memory not found: {memory_name}")
        sys.exit(1)

    content = md_file.read_text(encoding="utf-8")
    tracks = extract_tracks(content)
    if not tracks:
        print(f"{memory_name}: no tracks: field — skipping drift check.")
        sys.exit(0)

    modified = get_recent_modified_files(project_root, 30)
    matched = match_files_to_tracks(modified, tracks)
    if matched:
        print(f"{memory_name}: STALE — matched files: {', '.join(matched[:3])}")
        sys.exit(1)
    else:
        print(f"{memory_name}: up to date.")
        sys.exit(0)


def cmd_hook(_args: list[str]):
    """Stop hook mode — outputs JSON reminder only if session files match any tracks."""
    project_root = find_project_root()
    session_files = get_session_written_files(project_root)
    if not session_files:
        sys.exit(0)

    memories_dir = get_memories_dir(project_root)
    if not memories_dir.exists():
        sys.exit(0)

    stale_names = []
    for md_file in sorted(memories_dir.glob("*.md")):
        content = md_file.read_text(encoding="utf-8")
        tracks = extract_tracks(content)
        if not tracks:
            continue
        if match_files_to_tracks(session_files, tracks):
            stale_names.append(md_file.stem)

    if stale_names:
        names = ", ".join(stale_names[:3])
        suffix = f" (+{len(stale_names)-3} more)" if len(stale_names) > 3 else ""
        msg = f"[Docs Drift] {len(stale_names)} memor{'y' if len(stale_names)==1 else 'ies'} may need updating: {names}{suffix}. Run /documenting to review."
        output = {
            "hookSpecificOutput": {
                "hookEventName": "Stop",
                "additionalContext": msg,
            }
        }
        print(json.dumps(output))

    sys.exit(0)


# ── Entry point ───────────────────────────────────────────────────────────────

SUBCOMMANDS = {"scan": cmd_scan, "check": cmd_check, "hook": cmd_hook}


def main():
    args = sys.argv[1:]
    if not args or args[0] not in SUBCOMMANDS:
        print("Usage: drift_detector.py <scan|check|hook> [options]")
        print("  scan [--since N]    — scan all memories for drift (default N=30 commits)")
        print("  check <memory>      — check a single memory")
        print("  hook                — Stop hook mode (outputs JSON if stale)")
        sys.exit(1)

    SUBCOMMANDS[args[0]](args[1:])


if __name__ == "__main__":
    main()
```

**Step 4: Run tests**

```bash
cd skills/documenting && python -m pytest tests/test_drift_detector.py -v
```

Expected: all 6 pass.

**Step 5: Smoke test manually**

```bash
cd /home/dawid/projects/jaggers-agent-tools
python3 skills/documenting/scripts/drift_detector.py scan
```

Expected: output listing stale memories (none have `tracks:` yet → "All memories up to date").

**Step 6: Commit**

```bash
git add skills/documenting/scripts/drift_detector.py \
        skills/documenting/tests/test_drift_detector.py
git commit -m "feat(documenting): add drift_detector.py with scan/check/hook subcommands"
```

---

## Task 4: Add Stop hook to `config/settings.json`

**Files:**
- Modify: `config/settings.json`

**Step 1: Read current hooks section**

Open `config/settings.json` and locate the `"hooks"` object.

**Step 2: Add Stop hook entry**

Add `"Stop"` event alongside existing hooks:

```json
"Stop": [
  {
    "hooks": [
      {
        "type": "command",
        "command": "python3 \"$HOME/.claude/skills/documenting/scripts/drift_detector.py\" hook",
        "timeout": 5
      }
    ]
  }
]
```

**Step 3: Verify JSON is valid**

```bash
python3 -m json.tool config/settings.json > /dev/null && echo "valid"
```

Expected: `valid`

**Step 4: Commit**

```bash
git add config/settings.json
git commit -m "feat(documenting): add Stop hook for drift detection auto-trigger"
```

---

## Task 5: Migrate existing memories — add `tracks:` frontmatter

**Files:**
- Modify: all 11 memories in `.serena/memories/`

**Step 1: Determine track mappings for each memory**

| Memory | `tracks:` globs |
|---|---|
| `ssot_cli_hooks_2026-02-03` | `["hooks/**", "config/settings.json"]` |
| `ssot_cli_mcp_servers_2026-02-21` | `["config/mcp_servers*.json", "cli/src/**/*mcp*"]` |
| `ssot_cli_universal_hub_2026-02-19` | `["cli/src/**", "config/settings.json"]` |
| `ssot_cli_ux_improvements_2026-02-22` | `["cli/src/commands/**", "cli/src/core/**"]` |
| `ssot_cli_vault_2026-02-03` | `["config/settings.json", "cli/src/**/*vault*"]` |
| `ssot_jaggers-agent-tools_delegating_skill_2026-02-23` | `["skills/delegating/**", "hooks/skill-suggestion.py"]` |
| `ssot_jaggers-agent-tools_documenting_workflow_2026-02-03` | `["skills/documenting/**"]` |
| `ssot_jaggers-agent-tools_installer_architecture_2026-02-03` | `["cli/**", "config/**"]` |
| `ssot_jaggers-agent-tools_migration_2026-02-01` | `["cli/**"]` |
| `ssot_jaggers-agent-tools_orchestrating_agents_2026-02-03` | `["skills/orchestrating-agents/**"]` |
| `ssot_jaggers-agent-tools_service_skills_set_2026-02-23` | `["project-skills/service-skills-set/**"]` |

**Step 2: For each memory, open it and add `tracks:` after `domain:` line**

Example edit for `ssot_jaggers-agent-tools_service_skills_set_2026-02-23`:

```yaml
domain: "project-skills"        # existing
tracks:                          # add this
  - "project-skills/service-skills-set/**"
```

Use Serena `replace_content` with literal mode for each one:
```python
needle = 'domain: "project-skills"'
repl   = 'domain: "project-skills"\ntracks:\n  - "project-skills/service-skills-set/**"'
```

**Step 3: Validate all memories after migration**

```bash
for f in /home/dawid/projects/jaggers-agent-tools/.serena/memories/*.md; do
  python3 skills/documenting/scripts/validate_metadata.py "$f"
done
```

Expected: all pass validation + INDEX blocks generated in each.

**Step 4: Run drift scan to confirm detection works**

```bash
python3 skills/documenting/scripts/drift_detector.py scan --since 50
```

Expected: correctly lists memories as stale based on recent commits.

**Step 5: Commit**

```bash
git add .serena/memories/
git commit -m "chore(documenting): add tracks: frontmatter to all existing memories"
```

---

## Task 6: Rewrite SKILL.md workflow

**Files:**
- Modify: `skills/documenting/SKILL.md`

**Step 1: Replace the `## Workflows` section body**

Keep the mandatory `activate_project` first step. Replace the three numbered workflows with this decision-tree driven protocol:

```markdown
## Workflow

### Step 0: Activate project (MANDATORY)
```javascript
mcp__plugin_serena_serena__activate_project({ project: "/path/to/cwd" })
```

### Step 1: Detect drift
```bash
python3 "$HOME/.claude/skills/documenting/scripts/drift_detector.py" scan
```
Review output. If nothing stale and no explicit documentation request → confirm to user and stop.

### Step 2: Decide action

| Situation | Action |
|---|---|
| New feature shipped | Create new SSOT memory OR update existing |
| Refactor / architecture change | Update relevant SSOT, bump minor version |
| Bug fix only | CHANGELOG entry only (skip memory update unless behaviour changed) |
| SKILL.md drift flagged | Update skill + run `validate_metadata.py` on it |

### Step 3: Create or update memory

**Creating:**
```bash
python3 "$HOME/.claude/skills/documenting/scripts/generate_template.py" \
  ssot <name>_ssot.md title="..." domain="..." subcategory="..."
```
Fill `[PENDING]` placeholders. Add `tracks:` globs.

**Updating:**
1. Read INDEX block only — identify which sections to update
2. Edit stale sections using `search_for_pattern` to jump directly
3. Bump `version:` (patch = content fix, minor = new section, major = full rewrite)
4. Update `updated:` timestamp

### Step 4: Regenerate INDEX
```bash
python3 "$HOME/.claude/skills/documenting/scripts/validate_metadata.py" <memory-file>
```
This regenerates the `<!-- INDEX -->` block automatically.

### Step 5: Update CHANGELOG
```bash
python3 "$HOME/.claude/skills/documenting/scripts/changelog/add_entry.py" \
  <version> <type> "<summary>"
```
Types: `Added`, `Changed`, `Fixed`, `Removed`.
```

**Step 2: Update `description:` frontmatter**

```yaml
description: >-
  Maintain SSOT documentation with drift detection. Runs drift_detector.py scan
  on invoke to identify stale memories. Creates/updates Serena memories with
  auto-generated INDEX blocks. MUST be suggested after any feature, refactor,
  or architecture change is verified complete.
```

**Step 3: Commit**

```bash
git add skills/documenting/SKILL.md
git commit -m "feat(documenting): rewrite workflow with decision tree and drift scan step"
```

---

## Task 7: Run full test suite

**Step 1: Run all documenting tests**

```bash
cd /home/dawid/projects/jaggers-agent-tools
python -m pytest skills/documenting/tests/ -v
```

Expected: all tests pass.

**Step 2: End-to-end smoke test**

```bash
# 1. Generate a test memory
python3 skills/documenting/scripts/generate_template.py ssot \
  /tmp/test_smoke_ssot.md title="Smoke Test" domain="test" subcategory="test"

# Check INDEX stub is present
grep "INDEX" /tmp/test_smoke_ssot.md

# 2. Validate it (INDEX should regenerate)
python3 skills/documenting/scripts/validate_metadata.py /tmp/test_smoke_ssot.md

# 3. Run drift scan
python3 skills/documenting/scripts/drift_detector.py scan
```

**Step 3: Push**

```bash
git push
```

---

## Summary

| Task | Component | Type |
|---|---|---|
| 1 | `tracks:` schema + template stubs | Config/template |
| 2 | INDEX auto-generation in validate_metadata | New functions + tests |
| 3 | `drift_detector.py` | New script + tests |
| 4 | Stop hook in settings.json | Config |
| 5 | Migrate existing memories | Data migration |
| 6 | SKILL.md workflow rewrite | Docs |
| 7 | Full test suite + smoke test | Verification |
